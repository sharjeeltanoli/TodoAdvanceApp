# Implementation Plan: Event-Driven Todo System

**Branch**: `005-event-driven` | **Date**: 2026-02-17 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/005-event-driven/spec.md`

## Summary

Add event-driven architecture to the todo system using Redpanda (Kafka-compatible) as message broker and Dapr as the runtime abstraction. The system publishes task change events after each CRUD operation, enabling real-time client sync via SSE, due-date reminder notifications via cron-triggered checks, and automatic recurring task generation on completion. Two new microservices (Notification Service, SSE Gateway) join the existing backend, MCP server, and frontend — all communicating through Dapr sidecars on Kubernetes.

## Technical Context

**Language/Version**: Python 3.13 (backend, notification service, SSE gateway), TypeScript 5.x (frontend)
**Primary Dependencies**: FastAPI 0.115.8, Dapr SDK (HTTP API via httpx), Redpanda (Kafka-compatible), Next.js 16
**Storage**: Neon Serverless PostgreSQL (shared — adds `task_event`, `notification`, `processed_event` tables), Redis (Dapr state store for caching)
**Testing**: pytest (backend services), vitest (frontend)
**Target Platform**: Kubernetes (Minikube local, production-ready Helm charts)
**Project Type**: Web (monorepo — frontend/ + backend/ + services/)
**Performance Goals**: 100 events/sec throughput, <2s real-time sync latency (p99), <10s consumer lag
**Constraints**: At-least-once delivery, idempotent consumers, <256MB RAM per sidecar
**Scale/Scope**: 5 Kubernetes deployments (frontend, backend, MCP, notification, SSE gateway) + Redpanda + Redis

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

| Principle | Status | Notes |
|-----------|--------|-------|
| I. Spec-Driven Development | PASS | spec.md → plan.md → tasks.md workflow followed |
| II. Monorepo Structure | PASS | New services under `backend/services/` or top-level `services/`; shared config at root |
| III. Stateless Services | PASS | All state in PostgreSQL/Redis via Dapr state store; SSE connections are ephemeral |
| IV. Event-Driven Architecture | PASS | Core feature — Kafka via Dapr pub/sub for all async operations |
| V. User Isolation | PASS | Events carry `user_id`; SSE gateway filters by authenticated user; notifications scoped by user |
| VI. MCP Protocol | PASS | MCP server subscribes to events for AI context; no direct DB access from agents |
| Security (JWT auth) | PASS | SSE endpoint requires Bearer token; notification API requires auth; inter-service mTLS via Dapr |

## Project Structure

### Documentation (this feature)

```text
specs/005-event-driven/
├── plan.md              # This file
├── spec.md              # Feature specification
├── research.md          # Technology decisions and rationale
├── data-model.md        # New tables and event schemas
├── quickstart.md        # Local development setup guide
├── contracts/
│   ├── api.yaml         # New REST API endpoints (OpenAPI)
│   └── events.yaml      # Event schemas per topic
└── tasks.md             # Task list (generated by /sp.tasks)
```

### Source Code (repository root)

```text
backend/
├── app/
│   ├── main.py                    # [MODIFY] Register event publisher middleware
│   ├── models.py                  # [MODIFY] Add TaskEvent, Notification, ProcessedEvent models
│   ├── routes/
│   │   ├── todos.py               # [MODIFY] Publish events after CRUD operations
│   │   ├── notifications.py       # [NEW] Notification REST API (list, read, count)
│   │   └── history.py             # [NEW] Task event history endpoint
│   ├── events/
│   │   ├── __init__.py
│   │   ├── publisher.py           # [NEW] Dapr pub/sub publish helper
│   │   ├── schemas.py             # [NEW] Event dataclasses (TaskEvent, ReminderEvent, etc.)
│   │   └── handlers.py            # [NEW] Event subscriber handlers (recurring tasks)
│   ├── scheduler.py               # [MODIFY] Move to Dapr cron binding pattern
│   └── database.py                # [EXISTING] Shared async engine
│
├── services/
│   ├── notification/
│   │   ├── Dockerfile
│   │   ├── requirements.txt
│   │   ├── main.py                # [NEW] FastAPI app — subscribes to reminders topic
│   │   └── handlers.py            # [NEW] Create notifications, dedup logic
│   │
│   └── sse_gateway/
│       ├── Dockerfile
│       ├── requirements.txt
│       ├── main.py                # [NEW] FastAPI app — subscribes to task-updates topic
│       └── connections.py         # [NEW] SSE connection manager per user
│
├── mcp_server/
│   └── server.py                  # [MODIFY] Subscribe to task-events for AI context
│
├── alembic/
│   └── versions/
│       └── xxx_add_event_tables.py  # [NEW] Migration for task_event, notification, processed_event
│
└── Dockerfile.backend             # [EXISTING] No changes

dapr/
├── components/
│   ├── pubsub-redpanda.yaml       # [NEW] Kafka-compatible pub/sub component
│   ├── statestore-redis.yaml      # [NEW] Redis state store for caching
│   ├── cron-overdue-check.yaml    # [NEW] 5-minute cron binding
│   └── secrets-kubernetes.yaml    # [NEW] K8s secrets component
├── subscriptions/
│   ├── task-events-sub.yaml       # [NEW] Declarative subscription for task-events
│   ├── reminders-sub.yaml         # [NEW] Declarative subscription for reminders
│   └── task-updates-sub.yaml      # [NEW] Declarative subscription for task-updates
├── config.yaml                    # [NEW] Dapr config (tracing, mTLS, metrics)
└── resiliency.yaml                # [NEW] Retry policies and circuit breakers

chart/
├── values.yaml                    # [MODIFY] Add notification, SSE gateway, Redpanda, Redis
├── templates/
│   ├── notification-deployment.yaml    # [NEW]
│   ├── notification-service.yaml       # [NEW]
│   ├── sse-gateway-deployment.yaml     # [NEW]
│   ├── sse-gateway-service.yaml        # [NEW]
│   ├── redpanda-statefulset.yaml       # [NEW] Single-node Redpanda for local
│   ├── redpanda-service.yaml           # [NEW]
│   ├── redis-deployment.yaml           # [NEW] Redis for Dapr state store
│   ├── redis-service.yaml              # [NEW]
│   ├── dapr-components.yaml            # [NEW] All Dapr components bundled
│   ├── backend-deployment.yaml         # [MODIFY] Add Dapr sidecar annotations
│   ├── mcp-deployment.yaml             # [MODIFY] Add Dapr sidecar annotations
│   └── frontend-deployment.yaml        # [MODIFY] Add Dapr sidecar annotations

frontend/
├── src/
│   ├── lib/
│   │   └── sse.ts                      # [NEW] SSE client for real-time task updates
│   ├── components/
│   │   ├── notifications/
│   │   │   ├── notification-bell.tsx    # [NEW] Bell icon with unread count badge
│   │   │   └── notification-list.tsx    # [NEW] Dropdown notification panel
│   │   └── tasks/
│   │       └── task-list.tsx            # [MODIFY] Auto-refresh on SSE events
│   └── app/
│       └── dashboard/
│           └── page.tsx                 # [MODIFY] Wire SSE + notifications
│
├── Dockerfile.frontend                  # [EXISTING] No changes
```

**Structure Decision**: New microservices live under `backend/services/` to maintain monorepo structure (Constitution II). Dapr configuration gets its own top-level `dapr/` directory for kubectl apply workflow. Helm chart extends existing `chart/` with new templates.

## Architecture Overview

```
┌────────────┐     ┌─────────────────────────────────────────────────┐
│  Frontend   │────▶│  SSE Gateway  │◀── task-updates ──│            │
│  (Next.js)  │     │  (FastAPI)    │                   │            │
│             │     └───────────────┘                   │  Redpanda  │
│             │                                         │  (Kafka)   │
│  /api/*  ───│────▶┌───────────────┐                   │            │
│             │     │  Backend API  │── task-events ───▶│            │
│             │     │  (FastAPI)    │── reminders ─────▶│            │
│             │     │  + Dapr       │── task-updates ──▶│            │
└─────────────┘     │  sidecar      │◀── cron binding   │            │
                    └───────────────┘                   │            │
                                                        │            │
                    ┌───────────────┐                   │            │
                    │ Notification  │◀── reminders ─────│            │
                    │ Service       │                   │            │
                    │ (FastAPI)     │                   │            │
                    └───────────────┘                   │            │
                                                        │            │
                    ┌───────────────┐                   │            │
                    │  MCP Server   │◀── task-events ──│            │
                    │  (FastMCP)    │                   │            │
                    └───────────────┘                   └────────────┘
                                                              │
                    ┌───────────────┐                          │
                    │    Redis      │◀── Dapr state store ─────┘
                    └───────────────┘

                    ┌───────────────┐
                    │ Neon PostgreSQL│◀── All services (via asyncpg)
                    └───────────────┘
```

## Detailed Design

### 1. Kafka/Redpanda Setup

**Local (Minikube)**:
- Single-node Redpanda via Helm chart (`vectorized/redpanda`) or StatefulSet
- ~256MB RAM, single broker on port 9092
- 3 topics auto-created: `task-events`, `reminders`, `task-updates`
- Redpanda Console on port 8080 for topic inspection
- Retention: 7 days (matches spec assumption)

**Production (Redpanda Cloud)**:
- Managed Redpanda cluster — change only broker URL in Dapr component YAML
- SASL/PLAIN auth via Kubernetes secret
- TLS enabled

### 2. Dapr Components Design

Based on `.claude/skills/dapr-configuration/` templates:

| Component | Type | File | Scopes |
|-----------|------|------|--------|
| `pubsub` | `pubsub.kafka` | `pubsub-redpanda.yaml` | backend-api, notification-svc, sse-gateway, mcp-server |
| `statestore` | `state.redis` | `statestore-redis.yaml` | backend-api, notification-svc, mcp-server |
| `cron-overdue-check` | `bindings.cron` | `cron-overdue-check.yaml` | backend-api |
| `secrets` | `secretstores.kubernetes` | `secrets-kubernetes.yaml` | all |

**Dapr Config** (`config.yaml`):
- mTLS enabled (inter-service encryption)
- Tracing: 100% sampling → Jaeger collector in monitoring namespace
- Metrics: enabled with method + path labels
- API logging: enabled

**Resiliency** (`resiliency.yaml`):
- Pub/sub retry: exponential backoff, max 5 retries, 30s max interval
- Service invocation retry: constant 3s, max 3 retries
- Circuit breaker: trip after 3 consecutive failures, 30s timeout

### 3. Topic Architecture

| Topic | Partition Count | Publisher | Subscribers | Event Types |
|-------|----------------|-----------|-------------|-------------|
| `task-events` | 3 | backend-api | notification-svc, mcp-server, backend-api (recurring) | task.created, task.updated, task.deleted, task.completed |
| `reminders` | 1 | backend-api (cron) | notification-svc | reminder.upcoming, reminder.overdue |
| `task-updates` | 3 | backend-api | sse-gateway | sync.task-changed |

**Partitioning**: `task-events` and `task-updates` partitioned by `user_id` hash for ordering guarantee per user. `reminders` single partition (low volume).

**Consumer Groups**:
- `notification-svc-group`: notification service
- `sse-gateway-group`: SSE gateway
- `mcp-server-group`: MCP server
- `recurring-handler-group`: backend recurring task handler

### 4. Event Publishing in Backend

After each CRUD operation in `routes/todos.py`, publish events asynchronously:

```python
# backend/app/events/publisher.py
async def publish_task_event(event_type: str, task: Task, user_id: str, changed_fields: dict = None):
    """Publish to task-events AND task-updates topics via Dapr."""
    # 1. Persist to task_event table (audit trail)
    await save_task_event(event_type, task, user_id, changed_fields)

    # 2. Publish to task-events (full event for processing)
    await dapr_publish("task-events", f"task.{event_type}", {
        "event_type": event_type,
        "task_id": str(task.id),
        "user_id": user_id,
        "task": task_to_dict(task) if event_type != "deleted" else None,
        "changed_fields": changed_fields,
    })

    # 3. Publish to task-updates (lightweight for SSE sync)
    await dapr_publish("task-updates", "sync.task-changed", {
        "change_type": event_type,
        "task_id": str(task.id),
        "user_id": user_id,
        "changed_fields": list((changed_fields or {}).keys()),
        "timestamp": datetime.utcnow().isoformat(),
    })
```

**Integration points** in `todos.py`:
- `POST /api/todos` → `publish_task_event("created", task, user_id)`
- `PUT /api/todos/{id}` → `publish_task_event("updated", task, user_id, changes)`
- `DELETE /api/todos/{id}` → `publish_task_event("deleted", task, user_id)`
- `PATCH /api/todos/{id}/complete` → `publish_task_event("completed", task, user_id, {"completed": ...})`

**Async with retry**: Publishing is async (`asyncio.create_task`) — user request completes immediately (FR-003). The `dapr_publish()` helper retries failed publishes up to 3 times with exponential backoff (1s, 2s, 4s) before logging the failure. This ensures at-least-once delivery (FR-004) when the Dapr sidecar is temporarily unavailable.

### 5. Notification Service

**Location**: `backend/services/notification/`

**Responsibilities**:
- Subscribe to `reminders` topic → create `notification` records in PostgreSQL
- Dedup using Dapr state store: `reminder:{task_id}:{window}` key with TTL
- Expose REST API via Dapr service invocation (backend proxies `/api/notifications/*`)
- No direct external access — reached through backend API or Dapr invoke

**Endpoints**:
- `GET /dapr/subscribe` → programmatic subscription to `reminders`
- `POST /events/reminder` → handle reminder events
- `GET /notifications` → list user notifications
- `PATCH /notifications/{id}/read` → mark as read
- `POST /notifications/read-all` → mark all read
- `GET /notifications/unread-count` → badge count

### 6. SSE Gateway Service

**Location**: `backend/services/sse_gateway/`

**Responsibilities**:
- Subscribe to `task-updates` topic via Dapr pub/sub
- Maintain in-memory SSE connection registry (user_id → set of response streams)
- Fan out events to all connected clients for the matching user_id
- Auth: validate Bearer token on SSE connect (extract user_id)

**Connection Flow**:
1. Frontend connects: `GET /api/stream/tasks` with `Authorization: Bearer <token>`
2. Gateway validates token, registers connection for user_id
3. When `task-updates` event arrives for user_id, push to all registered streams
4. On disconnect, remove from registry

**Scaling**: Each gateway instance holds its own connections. All instances subscribe to the same topic (different consumer group partitions). Dapr fan-out ensures each instance gets all events for connected users.

### 7. Dapr Sidecar Injection

All Kubernetes deployments get Dapr annotations:

| Deployment | app-id | app-port | Components Used |
|------------|--------|----------|-----------------|
| backend-api | `backend-api` | 8000 | pubsub, statestore, cron-overdue-check, secrets |
| mcp-server | `mcp-server` | 8001 | pubsub, statestore, secrets |
| notification-svc | `notification-svc` | 8002 | pubsub, statestore, secrets |
| sse-gateway | `sse-gateway` | 8003 | pubsub, secrets |
| frontend | `frontend` | 3000 | (none — static serving, no sidecar needed) |

**Sidecar Resources**: 100m CPU request / 300m limit, 128Mi memory request / 256Mi limit.

### 8. Cron Binding for Overdue Checks

**Component**: `cron-overdue-check` binding, scoped to `backend-api`.

**Schedule**: `@every 5m`

**Handler** (`POST /cron-overdue-check` on backend):
1. Query tasks: `WHERE due_date <= now() + interval '1 hour' AND completed = false AND due_date IS NOT NULL`
2. For each qualifying task, check Dapr state store for dedup key `reminder:{task_id}:{window}`
3. If not sent: publish to `reminders` topic and set dedup key with 5-minute TTL (300s)
4. Separate query for overdue: `WHERE due_date < now() AND completed = false`
5. Publish overdue reminders (dedup key: `overdue:{task_id}:{date}`)

### 9. State Store Usage

Redis via Dapr `state.redis` component:

| Use Case | Key Pattern | TTL | Service |
|----------|-------------|-----|---------|
| Reminder dedup | `reminder:{task_id}:{window}` | 300s | backend-api |
| Overdue dedup | `overdue:{task_id}:{date}` | 86400s | backend-api |
| Conversation cache | `conversation:{id}` | 3600s | mcp-server |
| User preferences | `user-prefs:{user_id}` | 86400s | backend-api |

### 10. Service Invocation Patterns

Frontend → Backend → Other services via Dapr:

```
Frontend ──HTTP──▶ Backend API ──Dapr invoke──▶ Notification Service
                                               (GET /notifications)
                   Backend API ──Dapr invoke──▶ MCP Server
                                               (existing pattern)
```

Backend proxies notification endpoints to the Notification Service:
```python
# backend/app/routes/notifications.py
@router.get("/api/notifications")
async def list_notifications(user_id: str = Depends(get_current_user)):
    return await invoke_service("notification-svc", f"notifications?user_id={user_id}")
```

### 11. Local Development Workflow

```bash
# 1. Start Minikube with enough resources
minikube start --cpus=4 --memory=8192

# 2. Install Dapr on cluster
helm repo add dapr https://dapr.github.io/helm-charts/
helm install dapr dapr/dapr --namespace dapr-system --create-namespace --wait

# 3. Install Redpanda (single-node)
helm repo add redpanda https://charts.redpanda.com
helm install redpanda redpanda/redpanda --namespace default \
  --set statefulset.replicas=1 \
  --set resources.cpu.cores=1 \
  --set resources.memory.container.max=256Mi

# 4. Install Redis
helm install redis oci://registry-1.docker.io/bitnamicharts/redis \
  --set architecture=standalone \
  --set auth.enabled=false

# 5. Apply Dapr components
kubectl apply -f dapr/

# 6. Deploy app with Helm
helm upgrade --install todo-app ./chart -f chart/values.yaml

# 7. Verify
dapr status -k
kubectl get components.dapr.io
dapr dashboard
```

### 12. Production Setup

**Broker**: Redpanda Cloud (or Confluent Cloud)
- Change `dapr/components/pubsub-redpanda.yaml` broker URL to cloud endpoint
- Add SASL credentials via Kubernetes secret
- Enable TLS

**State Store**: Keep Redis (or upgrade to managed Redis)

**Scaling**:
- Backend API: 2-3 replicas
- Notification Service: 1-2 replicas
- SSE Gateway: 2+ replicas (scale with connection count)
- Redpanda: 3-node cluster (managed)

**Observability** (per `.claude/skills/observability-monitoring/`):
- Dapr config tracing → Jaeger
- Prometheus scrapes Dapr sidecar metrics on port 9090
- Grafana dashboards: pub/sub throughput, consumer lag, event processing latency
- Alerts: consumer lag > 10s (warning), dead-letter messages > 0 (critical)

## Complexity Tracking

| Violation | Why Needed | Simpler Alternative Rejected Because |
|-----------|------------|-------------------------------------|
| SSE Gateway uses in-memory connection registry (Principle III: Stateless Services) | SSE/WebSocket connections are inherently stateful — each open HTTP stream must be tracked to fan out events to the correct client. The registry is ephemeral (lost on restart) and not persisted state. | External connection registry (Redis) adds latency and complexity for a problem that auto-reconnect solves: when a pod restarts, clients reconnect within seconds via exponential backoff. The in-memory registry is the industry-standard pattern for SSE/WebSocket gateways. |

## Risks and Follow-ups

1. **Risk: Minikube resource pressure** — 5 services + Redpanda + Redis + Dapr on local cluster may need 8GB+ RAM. Mitigation: document minimum resource requirements; provide `values-minimal.yaml` with reduced replica counts.

2. **Risk: SSE connection scaling** — Each SSE gateway pod holds connections in memory. If a pod restarts, clients reconnect. Mitigation: frontend auto-reconnect with exponential backoff; consider sticky sessions via ingress.

3. **Follow-up**: Phase 5C (Cloud Deployment) will address production Redpanda Cloud setup, horizontal pod autoscaling, and managed Redis migration.
